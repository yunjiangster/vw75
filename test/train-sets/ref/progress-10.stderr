Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = train-sets/0001.dat
num sources = 1
average    since         example     example  current  current  current
loss       last          counter      weight    label  predict features
0.265372   0.265372           10        10.0   1.0000   0.1663       34
0.240389   0.215406           20        20.0   0.0000   0.2162      104
0.243411   0.249456           30        30.0   0.0000   0.3470       82
0.233421   0.203449           40        40.0   1.0000   0.4872       42
0.223978   0.186208           50        50.0   0.0000   0.2378       60
0.231688   0.270239           60        60.0   0.0000   0.3376      147
0.230530   0.223581           70        70.0   1.0000   0.4861      134
0.226160   0.195571           80        80.0   0.0000   0.2174      136
0.218144   0.154011           90        90.0   0.0000   0.2778      139
0.216603   0.202736          100       100.0   1.0000   0.3323       56
0.217835   0.230161          110       110.0   1.0000   0.5978       97
0.223529   0.286161          120       120.0   0.0000   0.4216      120
0.221081   0.191702          130       130.0   1.0000   0.4029       54
0.216509   0.157070          140       140.0   0.0000   0.4171       82
0.211236   0.137422          150       150.0   1.0000   0.3988      148
0.210661   0.202034          160       160.0   0.0000   0.6096       63
0.205254   0.118743          170       170.0   1.0000   0.7744       69
0.201144   0.131274          180       180.0   1.0000   0.7463       42
0.198207   0.145334          190       190.0   1.0000   0.7428       34
0.195090   0.135866          200       200.0   1.0000   0.5891       56

finished run
number of examples per pass = 200
passes used = 1
weighted example sum = 200
weighted label sum = 91
average loss = 0.19509
best constant = 0.455
best constant's loss = 0.247975
total feature number = 15482
